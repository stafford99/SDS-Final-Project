---
title: "SDS Certification Paper"
author: "Evelyn Stafford, ebs797"
date: "4/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As a supplement to my certification paper, I put together this R Markdown script with all of the code I used in my process. Below is a code chunk that I have commented up with descriptions of the steps I took. Thanks for reading!   

```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
  
## Including necessary packages 
library(dplyr)
library(tidyverse)
library(tidytext)
library(caret)
library(tm)

## Taking a glance at the pre-existing dataset with agencies labeled into one of the four categories 
agency_trainset <- readRDS('agency_trainset.RDS')

## Adding an id column to the dataset
agency_trainset <- tibble::rowid_to_column(agency_trainset, "id")

# Plotting the existing dataset to visualize the breakout of classes 
agency_trainset %>%
  ggplot(aes(agency_class)) +
  geom_bar()

# Tf-idf feature engineering approach___________________________________________
agency_counts <- map_df(1:2,
                      ~ unnest_tokens(agency_trainset, word, agency, 
                                      token = "ngrams", n = .x)) %>%
  anti_join(stop_words, by = "word") %>%
  count(id, word, sort = TRUE)

words_30 <- agency_counts %>%
  group_by(word) %>%
  summarise(n = n()) %>% 
  filter(n >= 30) %>%
  select(word) %>%
  na.omit()

agency_dtm <- agency_counts %>%
  right_join(words_30, by = "word") %>%
  bind_tf_idf(word, id, n) %>%
  cast_dtm(id, word, tf_idf)

agency_meta <- tibble(id = as.numeric(dimnames(agency_dtm)[[1]])) %>%
  left_join(agency_trainset[!duplicated(agency_trainset$id), ], by = "id")

# Setting a seed and creating the test set and the train set 
set.seed(1234)
trainIndex <- createDataPartition(agency_meta$agency_class, p = 0.8, list = FALSE, times = 1)

agency_df_train <- agency_dtm[trainIndex, ] %>% as.matrix() %>% as.data.frame()
agency_df_test <- agency_dtm[-trainIndex, ] %>% as.matrix() %>% as.data.frame()

response_train <- agency_meta$agency_class[trainIndex]
view(response_train)

trctrl <- trainControl(method = "none")

# Using the Naive-Bayes classification model on the tf-idf test and train sets
library("naivebayes")

nb_mod <- train(x = agency_df_train,
                y = as.factor(response_train),
                method = "naive_bayes",
                trControl = trctrl,
                tuneGrid = data.frame(laplace = 0,
                                      usekernel = FALSE,
                                      adjust = FALSE))

nb_pred <- predict(nb_mod,
                   newdata = agency_df_test)

# Confusion matrix to evaluate this feature engineering and model combination
nb_cm <- confusionMatrix(nb_pred, as.factor(agency_meta[-trainIndex, ]$agency_class))
nb_cm

# Using LogitBoost classification model on the tf-idf test and train sets
library("caTools")

logitboost_mod <- train(x = agency_df_train,
                        y = as.factor(response_train),
                        method = "LogitBoost",
                        trControl = trctrl)

logitboost_pred <- predict(logitboost_mod,
                           newdata = agency_df_test)

# Confusion matrix to evaluate this featuer engineering and model combination
logitboost_cm <- confusionMatrix(logitboost_pred, as.factor(agency_meta[-trainIndex, ]$agency_class))
logitboost_cm

# Using Neural Network classification model on tf-idf test and train sets
nnet_mod <- train(x = agency_df_train,
                  y = as.factor(response_train),
                  method = "nnet",
                  trControl = trctrl,
                  tuneGrid = data.frame(size = 1,
                                        decay = 5e-4),
                  MaxNWts = 5000)

nnet_pred <- predict(nnet_mod,
                     newdata = agency_df_test)

# Confusion matrix to evaluate this feature engineering and moodel combination
nnet_cm <- confusionMatrix(nnet_pred, as.factor(agency_meta[-trainIndex, ]$agency_class))
nnet_cm

})})
```


```{r error=FALSE, warning=FALSE, message=FALSE}
invisible({capture.output({
  
# One-hot encoding feature engineering approach_________________________________
# First enable Python in Rstudio
#library(reticulate)
#use_python("/usr/local/bin/python3.8")

#Install tensorflow and set up a virtual environment
#install.packages("remotes")
#install.packages("keras")
#remotes::install_github("rstudio/tensorflow")
#install_tensorflow(version = "2.8.0", method = "virtualenv", envname = "tf")
#library(tensorflow)
#use_condaenv("tf")
#sess = tf$compat$v1$Session()

#Load the libraries
#library(keras)
#library(tidyverse)
#library(stringr)
#library(tidytext)
#library(caret)
#library(dplyr)
#library(tm)

# Start one-hot encoding feature engineering approach
#max_features <- 100

#tokenizer <- text_tokenizer(num_words = max_features)

#tokenizer %>%
#  fit_text_tokenizer(text)

#tokenizer$document_count

#one_hot_results <- texts_to_matrix(tokenizer, text, mode = "binary") %>% as.data.frame() %>% cbind()
#one_hot_results <- tibble::rowid_to_column(one_hot_results, "id")

#data_engineered <- data2 %>% cbind(one_hot_results)

data_engineered <- readRDS('data_engineered.rds')
one_hot_results <- data_engineered[ -c(1:3) ]

# Setting a seed and creating the test set and the train set
set.seed(1234)
onehot_trainIndex <-sort(sample(nrow(data_engineered), nrow(data_engineered)*.8))
onehot_train <-  one_hot_results[onehot_trainIndex,] %>% as.matrix() %>% as.data.frame()
onehot_test <- one_hot_results[-onehot_trainIndex,] %>% as.matrix() %>% as.data.frame()
onehot_response_train <- data_engineered$agency_class[onehot_trainIndex]
onehot_trctrl <- trainControl(method = "none")

# Using Naive-Bayes classification model on one-hot encoding feature engineering test and train sets
onehot_nb <- train(x = onehot_train,
                  y = as.factor(onehot_response_train),
                  method = "naive_bayes",
                  trControl = onehot_trctrl,
                  tuneGrid = data.frame(laplace = 0,
                                        usekernel = FALSE,
                                        adjust = FALSE))
nb_pred_oh <- predict(onehot_nb,
                         newdata = onehot_test)

# Confusion matrix to evaluate this feature engineering and model combination 
nb_cm_oh <- confusionMatrix(nb_pred_oh, as.factor(data_engineered[-onehot_trainIndex, ]$agency_class))
nb_cm_oh

# Using LogitBoost classification model on one-hot encoding feature engineering test and train sets
onehot_logitboost <- train(x = onehot_train,
                        y = as.factor(onehot_response_train),
                        method = "LogitBoost",
                        trControl = onehot_trctrl)

logitboost_pred_oh <- predict(onehot_logitboost,
                           newdata = onehot_test)

# Confusion matrix to evaluate this feature engineering and model combination
logitboost_cm_oh <- confusionMatrix(logitboost_pred_oh, as.factor(data_engineered[-onehot_trainIndex, ]$agency_class))
logitboost_cm_oh

# Using Neural Network classification model on one-hot feature engineering test and train sets 
onehot_nnet <- train(x = onehot_train,
                  y = as.factor(onehot_response_train),
                  method = "nnet",
                  trControl = onehot_trctrl,
                  tuneGrid = data.frame(size = 1,
                                        decay = 5e-4),
                  MaxNWts = 5000)

nnet_pred_oh <- predict(onehot_nnet,
                     newdata = onehot_test)

# Confusion matrix to evaluate this feature engineering and model combination
nnet_cm_oh <- confusionMatrix(nnet_pred_oh, as.factor(data_engineered[-onehot_trainIndex, ]$agency_class))
nnet_cm_oh

})})
```

